{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CFRM521 Machine Learning in Finance\n",
    "## <center> Lecture 1\n",
    "### <center> Yoshihiro Shirai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning, Python basics, and creating a test set\n",
    "\n",
    "- These lecture notes are partially based on \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aurelien Geron (O'Reilly), 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview of the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start by some general definitions and terminology\n",
    "- We then explain how to setup Python and create a Jupyter notebook, which is the only acceptable format for submitting homework sets and the course project.\n",
    "- After that, the initial steps of a machine learning project is explained, particularly how to create a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Machine Learning Landscape\n",
    "- Read Chapter 1 of the textbook for an overview of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What is Machine Learning (ML)?\n",
    "\n",
    "**ML is the field of study that gives computers the ability to learn without being explicitly programmed**\n",
    "\n",
    "- **Example**: Consider a particular task, such as deciding if an email is spam.\n",
    "\n",
    "    - A non-ML approach is to maintain a database of email addresses corresponding to all past spam emails. Do you see a problem with this approach?\n",
    "\n",
    "    - A \"spam filter\" is a ML algorithm for flagging spam emails.\n",
    "\n",
    "    - Consider a set of known spam and non spam emails. Such a set for which the task (i.e. flagging spams) has been completed, is called a \"training set\"\n",
    "\n",
    "    - Consider a \"performance measure\" for the task of flagging spams. For example the percentage of correctly classified emails, which is called \"accuracy\"\n",
    "\n",
    "    - The spam filter is a ML algorithm because it learns from experience. That is, its performance, as measured by accuracy, improves after it is \"trained\" with a training set.\n",
    "\n",
    "- **Example**: Consider the task of playing a game.\n",
    "\n",
    "    - You come up with the optimal way of playing the game as a set of rules (possible for simple games such as XO, unrealistic for more complex games such as chess)\n",
    "\n",
    "    - You code the rules as (a possibly large number of) nested if-statements, which is capable of beating any opponent.\n",
    "\n",
    "    - Is your program an example of ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML performs well in the following scenarios:**\n",
    "\n",
    "- Tasks for which solutions exist but involve a lot of hand-tuning or long lists of rules (ML algorithm may be simple to implement and easier to maintain). Example: spam filtering\n",
    "\n",
    "- Complex problems for which there is no known solution. ML methods may be able to find good solutions in reasonable amount of time. Example: playing chess\n",
    "\n",
    "- Dynamic and evolving environments where the problem setting constantly changes. ML methods has the inherent ability to adapt to new data. Example: trading\n",
    "\n",
    "- Getting insights about the structure of complex systems and large data sets. By \"reverse engineering\" a trained ML algorithm, we may find out about the data used for training the algorithm. Example: Internet search engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Types of ML systems\n",
    "**There are three main categories of ML tasks/algorithms**:\n",
    "1. \"**supervised learning**:\" the training data includes the desired outcome (or \"label\") of the task at hand. Such data set is called \"labeled\".\n",
    "\n",
    "    - **Example**: we are given 10K pictures of dogs that have been digitized and turned into vectors representing the colors of each pixel. Each picture is labeled according to the breed, e.g. Bulldog, Huski, French Bulldog, Pug, Corgi, etc. We train our algorithm on these pictures with the hope that it will correctly detect the breed of a new picture.\n",
    "2. \"**unsupervised learning**:\" the training data is unlabeled.\n",
    "\n",
    "    - **Example**: You give 10K digitized pictures of different dogs (unlabeled, without the dog breed tag) to an algorithm in hope that it finds relationships or patterns that it deems most important. The algorithm may group the dogs according to their breed, or perhaps color. It may group the pictures according to something completely different from our \"human perspective.\" Unsupervised learning may be a bit difficult to grasp at first, think about it as the following task: \"here are the books in my bookshelf, please sort them.\"\n",
    "3. \"**reinforcement learning (RL)**:\" Staying with our \"dog theme,\" in reinforcement learning an algorithm learns to do a task by being rewarded for successful behavior and/or punished by unsuccessful behavior. There are plenty of examples in RL algorithms mastering different games (chess, GO, Atari games, etc.). There are exciting new applications, such as self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the three main categories, ML algorithm may also be characterized by other aspects. For example:\n",
    "\n",
    "1. How is new data used?\n",
    "\n",
    "    - \"**Online learning**\" is learning that occurs incrementally from a stream of data. So the model can be updated when new data comes in one instance at a time or in mini-batches without needing to refit from scratch.\n",
    "\n",
    "    - \"**Offline learning**\" or \"**batch learning**\" is when learning occurs using the full training data, not incrementally. So when new data comes in, the model is refit from scratch on the old and new training data.\n",
    "\n",
    "2. How does the system \"generalize\" from the training set to new data?\n",
    "\n",
    "    - \"**Instance-based learning**:\" The system uses a \"measure of similarity\" to relate the new data to relevant instances in the training set. Typical ML algorithms that fall in this category are K-nearest neighbors and thos based on kernel methods, used to determine clusters, rankings, and, more in general, in linear/nonlinear manifold learning \n",
    "\n",
    "    - \"**Model-based learning**:\" The system builds a model from the training set, and use it to make predictions. Neural networks are an example of model-based leanring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of data that are often encountered in machine learning:\n",
    "\n",
    "- **Cross-sectional data**: Data where each instance (row) represents an independent instance. Examples include a data set for house prices where each entry represents the features of a geographic area, or a data set for a medical treatment, where each entry represents the features of a person.\n",
    "\n",
    "- **Time series data**: Data in the form of a discrete sequence index by time, which can exhibit time dependencies. Examples include the series of monthly unemployment rates, or the daily returns of a stock price over some period of time.\n",
    "\n",
    "For now, we will consider only cross-sectional data. In later lectures, we will explain how time series data can be treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Python Basics\n",
    "\n",
    "- Python is a general-purpose interpreted high-level programing language.\n",
    "\n",
    "- Due to its readability, popularity, and numerous ready-to-use libraries, it has become the de facto programming language for scientific computing (and machine learning in particular).\n",
    "\n",
    "- If you have not used python before, start with [learnpython.org](https://learnpython.org/) or look at the [Python Tutorial](https://docs.python.org/3/tutorial/)\n",
    "\n",
    "- Our goal for this lecture is to go over a minimal Python setup which will be sufficient for our course. We will learn more Python programming as we progress through the course.\n",
    "\n",
    "- Start with installing Python by visiting the [Python website](https://www.python.org/).\n",
    "\n",
    "- You can also use other managed Python distribution such as [Anaconda](https://www.anaconda.com/). The libraries will be more stable and compatible. However, you will generally be working with older versions of the libraries. This is the easiest way to get started with Python.\n",
    "\n",
    "- We will mainly use a Python development environment called Jupyter, which is nice and minimalistic and suitable for interactive scientific computing. It is, however, not that efficient for more technical coding tasks such as proper debugging, or to use Git.\n",
    "\n",
    "- There are more sophisticated environments, for example, you can use [Visual Studio](https://code.visualstudio.com/).\n",
    "\n",
    "- Lastly, instead of (or in addition to) setting up your own Python environment, you can use one of the many free online servers. Check out, for example, [Google's Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each project typically requires installing specific libraries, and it is useful to create an isolated Python environment for each project, so that the libraries do not need to be reinstalled each time. This can be done using the `virtualenv` package. It creates a little \"overhead\" each time you are setting up a project, but will save you time in the long run.\n",
    "\n",
    "- In vscode: \n",
    "    - open the folder with your project\n",
    "    - run ctrl+shift+P to open the palette\n",
    "    - select 'Create environment'\n",
    "    - choose 'Conda' (highly suggested when you use Git) -> You should now see a '.conda' folder on the left\n",
    "    - for smaller projects, you can also choose venv (or type `python -m venv venv` in a new terminal) -> you will a 'venv' folder on the left\n",
    "        - in this case\" open new terminal and type: `.\\venv\\Scripts\\activate` -> you should see '(venv)' in terminal before the path\n",
    "    - now you can install new packages that will be accessible only through this envinroment by typing in the terminal `python -m pip install <package name>`\n",
    "    - note: \n",
    "        - if you use visual code, you need to add python to path environment variable, see [here](https://www.youtube.com/watch?v=4bUOrMj88Pc)\n",
    "        - To check that python is added to the path, you can run the command `python --version`\n",
    "        - installation of new packages in a virtual environment may require installation of openssl. I suggest:\n",
    "            - type: `choco install vscode-insiders.install`\n",
    "            - type: `choco install openssl --version=1.1.1.4`\n",
    "    - note for GitHub projects:\n",
    "        - running a jupyter notebook in a cloned repo requires installing ipykernel (`pip install ipykernel`) in the conda environment\n",
    "        - once installed (you can check by running `conda list`), close the folder and then reopen it and you should be able to run your notebook\n",
    "        - now you can save (ctrl+s) commit and sync (third icon on the leftmost menu) any change\n",
    "\n",
    "- If you use Anaconda\n",
    "    - open the Anaconda prompt \n",
    "    - use Python's own library manager `pip` to install the `virtualenv` package:\n",
    "    `pip install --upgrade virtualenv`\n",
    "    - Then, create a Python environment for this course:\n",
    "        `cd [some path]`\n",
    "        `virtualenv CFRM521`\n",
    "        `source [some path]/CFRM421/bin/activate # on Linux or macOS`\n",
    "        `\\[some path]\\CFRM421\\Scripts\\activate  # on Windows`\n",
    "\n",
    "- Install the following packages: `jupyter` (Python IDE), `numpy` and `scipy` (popular scientific libraries), `pandas` (data structures), `matplotlib` (plotting figures), `sklearn` (ML), `tensorflow` and `keras` (neural networks) by running:\n",
    "    - type `pip install --upgrade jupyter matplotlib numpy pandas scipy scikit-learn tensorflow keras`\n",
    "    - if you are using vscode, you should now see in the Lib folder of `venv` these packages\n",
    "\n",
    "- Create a new ipynb file in vscode, or open `Jupyter` by typing `jupyter notebook` and create a new notebook\n",
    "\n",
    "- Run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Anaconda: to close the notebook, run ctrl+C in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basics on GitHub**\n",
    "- You can sign up for free on GitHub\n",
    "\n",
    "- you'll need to install a few softwares on vscode - follow the instructions on https://code.visualstudio.com/docs/sourcecontrol/intro-to-git\n",
    "\n",
    "- create a new repo and upload any files you might already have\n",
    "\n",
    "- open vscode and sign up from the bash (create new terminal, select + icon in upper right file, and then select bash) by running \n",
    "    - `git config user.email \"youremail\"`\n",
    "    - `user config user.name \"yourname\"`\n",
    "\n",
    "- You should now be able to clone the repo on a local folder\n",
    "\n",
    "- GitHub allows mutliple people to collaborate on the same project, by granting access to the repo\n",
    "\n",
    "- Each collaborator will start by cloning the main branch of the repo on their local machine\n",
    "\n",
    "- When a collaborator wants to test something, they can type `git checkout -b branchname' to create a new branch from the main branch\n",
    "\n",
    "- Code can then be added, saved, committed, and synced to the new branch\n",
    "\n",
    "- Finally, the new branch can merged with main and the code incorporated in the project\n",
    "\n",
    "- All existing branches can be accessed by any collaborator, by typing `git --checkout track origin branchname\n",
    "\n",
    "- The list of existing branches can be accessed by typing `git branch -a\n",
    "\n",
    "- To check your current branch you can also type `git status`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example of creating a test set\n",
    "- Let us walk though setting up a simple ML project\n",
    "\n",
    "- Our goal here is to get familiar with a few Python packages\n",
    "\n",
    "- Read Chapter 2 of the book for the full description of the project\n",
    "\n",
    "- The task is to build a model of housing prices in California using California census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To begin with, we should frame the problem, choose a performance measure, obtain the data. The main steps for a machine learning project are:\n",
    "\n",
    "    1. Create a test set.\n",
    "\n",
    "    2. Clean the data, impute missing values, transform the variables.\n",
    "\n",
    "    3. Train some models to try out.\n",
    "\n",
    "    4. Fine-tune the models and select the best model.\n",
    "\n",
    "    5. Evaluate the performance of the final model on the test set.\n",
    "\n",
    "- Here, we focus on Step 1 and 2. In the next lectures, we will consider Steps 3 to 5. In Homework 1, you will go through an end-to-end machine learning project using all these steps.\n",
    "\n",
    "- There are other steps that may be useful. For example:\n",
    "\n",
    "    - On the training set, examine and visualize the data, perform some basic descriptive statistics to gain some insight.\n",
    "\n",
    "    - Use domain knowledge to find certain combinations of the variables (variables are also called attributes or features) that may be more useful for prediction.\n",
    "\n",
    "    - In the final model, use feature importance measures to examine which features are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Getting data\n",
    "- Let us start by obtaining the data, which can be downloaded from https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz\n",
    "\n",
    "- The data is a compressed CSV file. We can manually download and uncompress the file. A better method is to write a function for fetching the file (why is this method better?)\n",
    "\n",
    "- We start by loading relevant libraries that give us the functions we need to download and uncompress the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we define the function for downloading and uncompressing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, call the function and store its output in the variable housing which is of type `DataFrame`. Note that `Pandas` is a package for handling data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "type(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look at the top rows of housing by using the head() method\n",
    "\n",
    "- Each instance represents a block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes data, typically with a population of 600 to 3,000 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `info()` method gives a summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ocean_proximity` is a categorical attribute. We can find out more information about its categories by using the `value_counts()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get summary statistics by using the `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting histograms are a good way to get a quick visualization of the data. This can be achieved by using the `hist()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "scale = 2.75\n",
    "housing.hist(bins=50, figsize=(5*scale,2*scale), layout=(2,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Creating a test set\n",
    "- Before looking at the data any further, we must create a test set and put it aside for later use in approximating the **out-of-sample** or **generalization error**.\n",
    "\n",
    "- The underlying idea is as follows. Suppose that you have 10K labeled pictures of dogs. If you train a model using all 10K pictures, then how would you get an estimate of the error you would make in detecting a new picture?\n",
    "\n",
    "- You cannot use any measure of error based on the pictures used for training the algorithm. The algorithm has already \"seen\" those pictures. The analogy is a two-year old playing with her first jigsaw puzzle. Once she master the puzzle, does it mean that she has learned how to solve all types of jigsaw puzzles? Or maybe she only memorized her puzzle?\n",
    "\n",
    "- To be able to estimate the magnitude of the error a trained algorithm makes on a new observation, we split our data in two parts:\n",
    "    - A **training set** that is used for training the model.\n",
    "    - A **test set** that is used for estimating the out-of-sample error. The algorithm must NOT use the test set in ANY way during training.\n",
    "\n",
    "- Often a training set is further split off to make a **validation set**. This can be created in the same way as the test set, and is used for tuning hyperparameters and choosing which competing model has the best out-of-sample performance. We will discuss this later.\n",
    "\n",
    "- It is crucial to create the test set before you start thinking about the model to avoid \"*data-snooping bias*\".\n",
    "\n",
    "- This is a scenario where the model is overfitted to the data (including the test set). Although such model would seem to have a good performance on the existing data, it will not generally have a good performance on new data.\n",
    "\n",
    "- As a rule of thumb, pick 20% of your data as a test set.\n",
    "\n",
    "- We can use the `train_test_split()` function in Scikit-Learn to create a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By randomly choosing the test set, we may very well end up with highly biased samples\n",
    "\n",
    "- A systematic way to avoid this issue is to use \"stratified sampling.\"\n",
    "    \n",
    "- In this approach, the sample is divided into homogeneous subsamples called \"strata\". Then, an appropriate number of data points are taken from each stratum, so that the representation of each strata in the test set is maintained with respect to the original dataset\n",
    "\n",
    "- Assume that median income is a very important attribute in predicting median housing prices\n",
    "\n",
    "- We want to ensure that the test set is representative of various categories of income in the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we add a new attribute to the housing dataset, called \"income_cat\" and whose values range from 1 to 5\n",
    "\n",
    "- Each value of the new attribute represents a level of income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us see how the income categories are populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0,grid=True)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of block groups\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now use Scikit-Learn's `StratifiedShuffleSplit` class to create a stratified sampling based on the income category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us check the distribution of income categories in the resulting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let us compare with the distribution of the income categories in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us compare the sampling bias between random and stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to re-create the random test sample, since we added the income categories\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * \\\n",
    "    compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * \\\n",
    "    compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should remove the `income_cat` attribute from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We save a copy of the training set for exploring and building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
